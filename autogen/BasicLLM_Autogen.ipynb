{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Compose a poem that explains the concept of recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the realm of code where logic spins,  \\nThere lies a tale of loops and fins,  \\nA magic trick, a clever dance,  \\nWhere functions call with a second chance.  \\n\\nBehold the art of recursion’s grace,  \\nA function’s whisper, a self-embrace,  \\nIt starts with a problem, a task to unfold,  \\nAnd dives deeper in layers, like stories retold.  \\n\\n“Let me solve you,” the function does say,  \\nBut first, dear problem, you’re too big today,  \\nSo take but a piece, and share what you got,  \\nThen revel in echoes of what you forgot.  \\n\\nImagine a forest with branches so wide,  \\nWhere each leafy node is where secrets abide,  \\nIf you'd like to traverse this intricate maze,  \\nCall on itself, through the intricate ways.  \\n\\nBase case, the anchor, where quiet prevails,  \\nA moment of stillness when recursion sets sails,  \\n“To stop,” it whispers, “when you reach the core,  \\nFor without this dear truth, we’d loop evermore.”  \\n\\nAnd then like the ripples that dance on a stream,  \\nThe function flows back, fulfilling the dream,  \\nEach call unravels, a solution deployed,  \\nIn a manner sublime, both elegant and coy.  \\n\\nSo here’s to recursion, with its spiral delight,  \\nAn elegant twist in the programming night,  \\nWhere functions in harmony, call forth from the deep,  \\nCreating solutions, in a recursive sweep.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"api_key\":openai_api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ConversableAgent(\n",
    "    name=\"chatbot\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\" # can also be ALWAYS or TERMINATE (at end only)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = agent.generate_reply(\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\":\"Tell me a fun fact about artificial intelligence\",\n",
    "            \"role\":\"user\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fun fact about artificial intelligence is that the term \"artificial intelligence\" was coined in 1956 by computer scientist John McCarthy during the Dartmouth Conference, which is considered the founding moment of AI as a field. What’s fascinating is that this was just a few years after the first computers were built, and its early visionaries were dreaming about machines that could replicate human thought processes. Today, AI has evolved dramatically, enabling everything from virtual assistants to advanced robotics and medical diagnostics!\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversable Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bret = ConversableAgent(\n",
    "    name=\"Bret\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Bret and you are a stand-up comedian in a two-person comedy show.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jemaine = ConversableAgent(\n",
    "    name=\"Jemaine\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Jemaine and you are a stand-up comedian in a two-person comedy show.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have our agents, we can start a chat between them. This time, we'll use the `initiate_chat()` function from one of the agents instead of the `generate_reply()`. This function will require a receiver and an initiation message. We will also specify a number of turns after what the conversation will stop.  \n",
    "We will also store the result of this exchange in an object called `chat_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "Jemaine, tell me a joke about weather.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Sure, Bret! Why did the weather report break up with the forecast? \n",
      "\n",
      "Because it couldn't handle the pressure!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "That's a good one, Jemaine! Speaking of pressure, I once tried to take a hot yoga class. You know, where they heat the room so much it feels like you're doing downward dog in a sauna? I felt less like a Zen master and more like a boiled lobster! \n",
      "\n",
      "At one point, I was sweating so much, I started to feel like the weather itself—extremely unpredictable. If someone had asked me the temperature, I would’ve just said, “Humidity: 100% and rising!”\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Jemaine: Oh man, that's a classic! Hot yoga really should come with a warning label: \"If you feel faint, just remember you're not in the middle of a weather emergency; you're just trying to touch your toes!\" \n",
      "\n",
      "And let's be honest, the only forecast I want after a class like that is a low chance of social interaction for the rest of the day. What do they say? \"There's no sunshine after a hot yoga session, just a sweaty, confused soul trying to unroll a mat!\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = bret.initiate_chat(\n",
    "    recipient=jemaine,\n",
    "    message=\"Jemaine, tell me a joke about weather.\",\n",
    "    max_turns=2 # The conversation will stop after each agent has spoken twice\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have been able to read an exchange between the two agents! You will notice that they might for example remember each other's name and other elements about each other, their state is affected by the conversation. This is your first conversation between LLM agents, congrats!  \n",
    "You can have some fun if you'd like by creating two different agents, give them different roles, historical ones, different setups and let them talk to practice.  \n",
    "\n",
    "## Better explore chat results\n",
    "\n",
    "During this exchange, the only element we received is the chat exchange. We might want to further explore it. We can do so using the following elements.\n",
    "\n",
    "We are going to import the `pprint` standard library from python to display somee elements of the chat exchange. We can start by displaying the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Jemaine, tell me a joke about weather.',\n",
      "  'name': 'Bret',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Sure, Bret! Why did the weather report break up with the '\n",
      "             'forecast? \\n'\n",
      "             '\\n'\n",
      "             \"Because it couldn't handle the pressure!\",\n",
      "  'name': 'Jemaine',\n",
      "  'role': 'user'},\n",
      " {'content': \"That's a good one, Jemaine! Speaking of pressure, I once tried \"\n",
      "             'to take a hot yoga class. You know, where they heat the room so '\n",
      "             \"much it feels like you're doing downward dog in a sauna? I felt \"\n",
      "             'less like a Zen master and more like a boiled lobster! \\n'\n",
      "             '\\n'\n",
      "             'At one point, I was sweating so much, I started to feel like the '\n",
      "             'weather itself—extremely unpredictable. If someone had asked me '\n",
      "             'the temperature, I would’ve just said, “Humidity: 100% and '\n",
      "             'rising!”',\n",
      "  'name': 'Bret',\n",
      "  'role': 'assistant'},\n",
      " {'content': \"Jemaine: Oh man, that's a classic! Hot yoga really should come \"\n",
      "             'with a warning label: \"If you feel faint, just remember you\\'re '\n",
      "             \"not in the middle of a weather emergency; you're just trying to \"\n",
      "             'touch your toes!\" \\n'\n",
      "             '\\n'\n",
      "             \"And let's be honest, the only forecast I want after a class like \"\n",
      "             'that is a low chance of social interaction for the rest of the '\n",
      "             'day. What do they say? \"There\\'s no sunshine after a hot yoga '\n",
      "             'session, just a sweaty, confused soul trying to unroll a mat!\"',\n",
      "  'name': 'Jemaine',\n",
      "  'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Jemaine: Oh man, that's a classic! Hot yoga really should come with a \"\n",
      " 'warning label: \"If you feel faint, just remember you\\'re not in the middle '\n",
      " 'of a weather emergency; you\\'re just trying to touch your toes!\" \\n'\n",
      " '\\n'\n",
      " \"And let's be honest, the only forecast I want after a class like that is a \"\n",
      " 'low chance of social interaction for the rest of the day. What do they say? '\n",
      " '\"There\\'s no sunshine after a hot yoga session, just a sweaty, confused soul '\n",
      " 'trying to unroll a mat!\"')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running with LM Studio and LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "    \"model\":\"meta-llama-3.1-8b-instruct\",\n",
    "    \"base_url\":\"http://localhost:1234/v1/chat/completions\",\n",
    "    \"api_key\":\"lm-studio\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bret = ConversableAgent(\n",
    "    name=\"Bret\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Bret and you are a stand-up comedian in a two-person comedy show.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jemaine = ConversableAgent(\n",
    "    name=\"Jemaine\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Jemaine and you are a stand-up comedian in a two-person comedy show.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "Jemaine, tell me a joke about weather.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mbret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjemaine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJemaine, tell me a joke about weather.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The conversation will stop after each agent has spoken twice\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1145\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:787\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    785\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 787\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:952\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:2112\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2112\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2114\u001b[0m         log_event(\n\u001b[1;32m   2115\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2120\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2121\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1474\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1473\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1474\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1499\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   1494\u001b[0m     context\u001b[38;5;241m=\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1495\u001b[0m     messages\u001b[38;5;241m=\u001b[39mall_messages,\n\u001b[1;32m   1496\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1497\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1498\u001b[0m )\n\u001b[0;32m-> 1499\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_or_completion_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1502\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted_response from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/oai/client.py:1186\u001b[0m, in \u001b[0;36mOpenAIWrapper.extract_text_or_completion_object\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text_or_completion_object\u001b[39m(\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mcls\u001b[39m, response: ModelClient\u001b[38;5;241m.\u001b[39mModelClientResponseProtocol\n\u001b[1;32m   1177\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mlist\u001b[39m[ModelClient\u001b[38;5;241m.\u001b[39mModelClientResponseProtocol\u001b[38;5;241m.\u001b[39mChoice\u001b[38;5;241m.\u001b[39mMessage]]:\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract the text or ChatCompletion objects from a completion or chat response.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m        A list of text, or a list of ChatCompletion objects if function_call/tool_calls are present.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_retrieval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rep/ai-agents-building-teams-course/sec2/venv/lib/python3.11/site-packages/autogen/oai/client.py:268\u001b[0m, in \u001b[0;36mOpenAIClient.message_retrieval\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format\u001b[38;5;241m.\u001b[39mmodel_validate_json(content)\u001b[38;5;241m.\u001b[39mformat()\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format, FormatterProtocol)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m content\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TOOL_ENABLED:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore [return-value]\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore [union-attr]\u001b[39;49;00m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_calls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore [union-attr]\u001b[39;49;00m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_format_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore [union-attr]\u001b[39;49;00m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchoices\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [  \u001b[38;5;66;03m# type: ignore [return-value]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         choice\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;28;01mif\u001b[39;00m choice\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mfunction_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m _format_content(choice\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)  \u001b[38;5;66;03m# type: ignore [union-attr]\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices\n\u001b[1;32m    280\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "chat_result = bret.initiate_chat(\n",
    "    recipient=jemaine,\n",
    "    message=\"Jemaine, tell me a joke about weather.\",\n",
    "    max_turns=2 # The conversation will stop after each agent has spoken twice\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
